<!DOCTYPE html>
<html>
	<head>
		<script src="/bjc-r/llab/loader.js"></script>
		<title>Algorithmic Bias and the Moral Machine</title>
	</head>
	<body>
		<h2>Evaluating your Algorithm</h2>
		<p>
			Now, it's time to evaluate your algorithm. 
		</p>
		<p class="alert alert-success quoteGreen"> 
			First, rank the possible groups from 1-13 based on how you 
			think your algorithm favors them: 1 means that they are most likely
			to be saved in any scenario, and 13 means that they are least
			likely to be saved in any scenario. 
		</p>
		<p>
			Here are the groups:
		</p>
		<ul>
			<li>Cats</li>
			<li>Dogs</li>
			<li>Babies</li>
			<li>Children</li>
			<li>Adults</li>
			<li>Elderly People</li>
			<li>Doctors</li>
			<li>CEOs</li>
			<li>Criminals</li>
			<li>Unemployed People</li>
			<li>Men</li>
			<li>Women</li>
			<li>Pregnant People</li>
		</ul>
		<p class="alert alert-success quoteGreen">
			Now, let's see how your algorithm really behaves. We've built an auditor
			that runs 10000 different scenarios and evaluates how likely each group
			is to survive. Run <code>python3 auditor.py</code> to see the results! Please make sure to run this using Python 3. 
		</p>
		<p>
			Now that you've seen the results of your algorithm, answer these questions
			with your group members:
		</p>
		<ul>
			<li>Did your algorithm behave as you expected? What surprised you? Why
			do you think this happened?</li>
			<li>Do you see any of your personal biases in the way your algorithm behaves? 
			How might you avoid having your biases present in your code?</li>
			<li>How do you think self-driving cars should really handle situations like this?
			Should a group of engineers write an algorithm like you did, or is there a different
			way to go about this?</li>
		</ul>
	</body>
</html>