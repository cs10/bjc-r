<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="content-type" content="text/html; charset=utf-8">
		<script src="/bjc-r/llab/loader.js"></script>
		<title>Lab 3: Timing Experiments</title>
	</head>

	<body>
    	<h2>Constant-Time</h2>
		<div class="todo">
            <ul>
                <li>This was originally two pages that I combined. --MF</li>
            </ul>
		</div>

		<p>Let us start our timing experiments with something simple: how long does it take for a computer to <em>add 1 to</em> (that is, <em>increment</em>) a number?</p>
        <div class="forYouToDo" id="first">
            <ol>
                <li>Replace "Hello!" in the <a href="/bjc-r/cur/programming/5-algorithms/3-timing-experiments/1-time-is-of-the-es-sense.html" target="_blank">timing script</a> with an addition operator from the "Operators" palette, and add 1 to 100,000.</li>
                <li>Run the script a few times (around four) to get an idea of the average approximate time (in seconds) it takes for the computer to increment 100,000.</li>
                <li>Run the script repeatedly. (Do <em>not</em> place the script inside a <code>repeat</code> or a <code>repeat until</code> block, since we are interested in knowing the <em>approximate</em> time the script takes to run <em>once</em>.)</li>
                <li><img class="inline" src="/bjc-r/img/icons/talk-with-your-partner.png" alt="Talk with Your Partner" title="Talk with Your Partner" /> What's your gut feeling for how much longer the computer would take if we doubled 100,000?</li>
                <li>
                    Test your intuition: Create a table of the an average approximate times for how long it takes the computer to increment (add 1 to) numbers that you progressively double:
                    <ul>
                        <li>100,000</li>
                        <li>200,000</li>
                        <li>400,000</li>
                        <li>800,000</li>
                        <li>1,600,000</li>
                    </ul>
                    Remember that for each number, you need to run the timing script multiple times (around four) to get an idea of the <em>average</em> approximate time (you don't have to be precise).
                </li>
                <li><img class="inline" src="/bjc-r/img/icons/talk-with-your-partner.png" alt="Talk with Your Partner" title="Talk with Your Partner" /> What do you observe? </li>
            </ol>
        </div>
        <div class="takeItFurther">
    		<img class="imageRight" src="/bjc-r/img/icons/tough-stuff-mini.png" alt="Tough Stuff" title="Tough Stuff" />
            <ol type="A">
        		<li>
            		Take another huge leap and find out approximately how long it takes for the computer to increment numbers that you progressively scale by 10: 
                    <ul>
                        <li>160,000,000</li>
                        <li>1,600,000,000</li>
                        <li>16,000,000,000</li>
                    </ul>
                    What do you observe? How is this similar to or different from scaling by 2?
                </li>
            </ol>
        </div>
        
        <div class="todo">MF: The section below is too long. Let's  trim it down considerably, especially the third paragraph.</div>
        <p>In the timing experiments above, you may have noticed that the computer takes approximately the same time to increment a number, even though the number was made progressively larger. This is why, computer scientists call incrementing a number a <em>constant-time</em> operation. It turns out, that <em>any</em> basic arithmetic operation (addition, subtraction, multiplication, division, and exponentiation) is considered to be a constant-time operation.</p>
        <p>Notice that we call these operations <em>constant-time</em> operations, but we don't actually say how much time they take because different computers will take different amounts of time to perform the same operation. Instead, <em>we focus on how the running time of an algorithm scales as we scale its inputs to larger and larger sizes</em> because this is a property of the algorithm itself, and it is independent of the computer that it is run on.</p>
        <p>Why did the computer take approximately the same amount of time to increment a number, even though that number was getting larger? Think about how you would add one to a number back in your elementary school days; this is similar to how a computer does its arithmetic (ignoring technical details). The elementary school way of adding numbers goes digit by digit, and so the amount of time it takes for you to add two numbers depends on how many digits each number has. As we doubled the number we were incrementing, we didn't consistently add digits to it, and so the computer took approximately the same time. Even as we began scaling the number by ten, the computer (and you!) takes a relatively small amount of time to account for the extra digit, so the total time remains approximately constant.</p>
        <p>Constant-time operations are the Holy Grail of computer science algorithms, and unfortunately, most algorithms are <em>not</em> constant-time...</p>   


	</body>
</html>