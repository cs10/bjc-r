<!DOCTYPE html>
<html>
    <head>
        <script src="/bjc-r/llab/loader.js"></script>
        <title>Building a Markov Model</title>
    </head>
    <body>
        <p>For this part of the lab, create a new file markov.py. Your ultimate goal is to write a function build_markov_model(text, k) that generates a dictionary representation of a Markov Chain of order k for the given text. For example, build_markov_model("") will return

        In it, you'll want to create the functions listed below.
        </p>

<pre>
<code>def get_kgram(text, ptr, k):
    """Returns the kgram starting at position ptr in a text. For example:

    get_kgram("hello", 0, 3) would return "hel"
    get_kgram("hello", 1, 3) would return "ell"
    get_kgram("hello", 3, 4) would return "lohe"

    Hint: Create a new copy of the text that is just the text repeated twice. This is inefficient but will make your life easy."""</code>
    </pre>

<pre>
<code>def get_kgram(text, ptr, k):
    """Returns the kgram starting at position ptr in a text. For example:

    get_kgram("hello", 0, 3) would return "hel"
    get_kgram("hello", 1, 3) would return "ell"
    get_kgram("hello", 3, 4) would return "lohe"

    Hint: Create a new copy of the text that is just the text repeated twice. This is inefficient but will make your life easy."""</code>
    </pre>

        <p>C8 and CS61A don't overlap very much. Both assume no prior programming experience (not even CS10), but CS61A focuses much more on traditional computer science topics. By contrast C8 focuses more on ...
        </p>

        <h3>Building an AI that Can Talk (Optional)</h3>

        <p>While this exercise is optional, it's definitely within reach of everyone in CS10. We encourage you to try this activity, though if you don't have the time that's fine. This exercise is great practice with Python data structure, so you might consider doing it while you're studying for finals, even if you don't do it the same week as the rest of this lab.

        <p>So far, we've talked about "natural language processing". We feed our algorithms some text data, and it spits back out something else. The reminder of this lab is an exercise in "natural language generation", where we'll train a model how to speak and have it generate new text for us. We'll use the simplest possible model, but the results may still surprise you in their sophistication.
        </p>

        <p>Let's start with some terminology. Given a text, the set of k-grams in that text is the set of all texts of length k that appear in that text. For example, if our text is "the ether" and k = 3, then the 3-grams that appear are "the", "he ", "e e", " et", "eth", "her", "ert", and "rth". The mysterious last two 3grams are "ert" and "rth" because we pretend that the text wraps around. You could also build a markov model that does not involve wrapping the text like this, but it'll be handy for our ultimate goals (explained later).
        </p>

        <p>The model that we'll be building of our text is a Markov Chain. A Markov Chain is a table of information about how frequently particular characters follow each unique kgram appearing in the text. As an example, the Markov Chain for our text with k=3 is:
        </p>

<pre>                frequency of           probability that
                 next char               next char is 
kgram   freq         e   h   r   t         e   h   r   t
-----------------------------------------------------------
 the      2      1   0   0   1   0    1/2  0   0  1/2  0 
 he       1      0   1   0   0   0     0   1   0   0   0  
 e e      1      0   0   0   0   1     0   0   0   0   1
  et      1      0   0   1   0   0     0   0   1   0   0 
 eth      1      0   1   0   0   0     0   1   0   0   0
 her      1      0   0   0   0   1     0   0   0   0   1
 ert      1      0   0   1   0   0     0   0   1   0   0
 rth      1      0   1   0   0   0     0   1   0   0   0</pre>

<p>Each kgram in the sentence appears exactly once, except for "the", which appears twice: Once at the beginning, and once in the middle of the word ether. Among the two times that "the" appears, one time it is followed by a space, and the other time it is followed by an r.</p>

<p>Recall that we treat our text as circular, i.e. we pretend that the string wraps around back to the beginning after the end. This results in the 3grams "ert" and "rth". These kgrams start at the 2nd to last and last character of the string respectively.</p>

<p>


def kgram(text, ptr, k):
    return text[ptr:ptr+k]

def process_character(m, text, ptr, k):
    """Adds information about the given character to the model

    m is the model (a dictionary)
    text is the text
    text[ptr] is the character to be processed
    k is the order of our Markov chain"""
    
    this_kgram = kgram(text, ptr, k)
    next_character = text[ptr+k]

    if this_kgram not in m:
        m[this_kgram] = {}

    if next_character not in m[this_kgram]:
        m[this_kgram][next_character] = 1
    else:
        m[this_kgram][next_character] += 1

def build_markov_model(text, k):
    m = {}
    text_length = len(text) - k
    for ptr in range(0, text_length):
        process_character(m, text, ptr, k)

    return m

def random_character(m, kgram):
    """returns a character that follows the given kgram"""
    char_list = []
    for char, count in m[kgram].items():
        char_list.extend([char] * count)
    
    return random.choice(char_list)

def generate_random_text(m, N):
    starter = random.choice(list(m.keys()))

#    if starter not in m.keys():
#        return("Invalid starter")
    
    output_str = starter
    current_kgram = starter

    for i in range(N):
        new_char = random_character(m, current_kgram)
        output_str = output_str + new_char
        current_kgram = current_kgram + new_char
        current_kgram = current_kgram[1:]

    return output_str

    </body>
</html>